# -*- coding: utf-8 -*-
"""retail_banking.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TL7B89vLkRH7S4OmxoU2j9lWcqLrh4Rc

### Additional Resources:
1. Exploratory Data Analysis:

https://www.youtube.com/watch?v=xi0vhXFPegw&t=1925s

2. Machine Learning:

https://www.youtube.com/playlist?list=PLTKMiZHVd_2KyGirGEvKlniaWeLOHhUF3


https://www.ibm.com/think/machine-learning#605511093


https://alan-turing-institute.github.io/Intro-to-transparent-ML-course/index.html


https://junaidsqazi.medium.com/ (A series)

https://medium.com/data-science/class-imbalance-strategies-a-visual-guide-with-code-8bc8fae71e1a (Handling Class Imbalanaces)

Import modules
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.metrics import mean_squared_error, confusion_matrix, roc_curve, auc, classification_report
from sklearn.preprocessing import StandardScaler

"""Load the loan data into a pandas dataframe"""

input_file_path = "/content/drive/MyDrive/Tangible/DataSet17Dec2025.xlsx"
df = pd.read_excel(input_file_path, sheet_name=1)

"""Only use the latest entry for each customer"""

df['AS_ON'] = pd.to_datetime(df['AS_ON'])

# Sort by ID and Date (Ascending), then keep the LAST one (Latest date)
df = df.sort_values(by=['MAPPED_FORACID', 'AS_ON'], ascending=[True, True])
initial_count = len(df)
df = df.drop_duplicates(subset=['MAPPED_FORACID'], keep='last')

print(f"Original Records: {initial_count}")
print(f"Unique Accounts (Latest Month Only): {len(df)}")
df.columns

numeric_cols = df.select_dtypes(include=np.number).columns
categorical_cols = df.select_dtypes(include='object').columns
for col in numeric_cols:
    if df[col].isnull().any():
        df[col] = df[col].fillna(0)
        print(f"Filled NaN in numeric column '{col}' with 0.")

# Handle Missing Categorical Values: Replace NaN with 'Unknown'
for col in categorical_cols:
    if df[col].isnull().any():
        df[col] = df[col].fillna('Unknown')
        print(f"Filled NaN in categorical column '{col}' with 'Unknown'.")
print(categorical_cols)
# Group all the categories with less than 1% data into a single group
for col in categorical_cols:
    if col in ['CUST_DOB', 'MAPPED_FORACID', 'MAPPED_CIF_ID', 'ACCT_OPN_DATE', 'AS_ON']: continue
    value_counts = df[col].value_counts(normalize=True)
    sparse_categories = value_counts[value_counts < 0.01].index
    if not sparse_categories.empty:
        df[col] = df[col].replace(sparse_categories, 'Other')
        print(f"Grouped sparse categories in '{col}' into 'Other'.")

df['loan_active'] = ((df['FONELOAN_OS'] > 0) | (df['CC_OS'] > 0)).astype(int)
print(df['loan_active'].value_counts())

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(15, 8))
sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Values Heatmap')
plt.show()

df.shape
df.columns

"""## Univariate Analysis"""

# Re-identify numerical and categorical columns after adding 'loan_active'
numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
categorical_cols = df.select_dtypes(include='object').columns.tolist()

# Exclude identifier columns and date columns if they are still in numerical or categorical lists
exclude_cols = ['MAPPED_FORACID', 'MAPPED_CIF_ID', 'ACCT_OPN_DATE', 'CUST_DOB', 'AS_ON']
numeric_cols = [col for col in numeric_cols if col not in exclude_cols]
categorical_cols = [col for col in categorical_cols if col not in exclude_cols]

print("Numerical Columns:", numeric_cols)
print("Categorical Columns:", categorical_cols)

"""### Univariate Analysis: Numerical Features"""

print("Descriptive Statistics for Numerical Features:")
display(df[numeric_cols].describe().T)

# Plot histograms for numerical features
fig, axes = plt.subplots(len(numeric_cols), 1, figsize=(10, 5 * len(numeric_cols)))
axes = axes.flatten()

for i, col in enumerate(numeric_cols):
    sns.histplot(df[col], kde=True, ax=axes[i])
    axes[i].set_title(f'Distribution of {col}')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('Frequency')

plt.tight_layout()
plt.show()

"""### Univariate Analysis: Categorical Features"""

print("Value Counts for Categorical Features:")
for col in categorical_cols:
    print(f"\n--- {col} ---")
    display(df[col].value_counts(normalize=True))

    # Plot bar plots for categorical features (top 10 if many unique values)
    plt.figure(figsize=(10, 6))
    sns.countplot(y=col, data=df, order=df[col].value_counts().index[:10], palette='viridis')
    plt.title(f'Count of {col}')
    plt.xlabel('Count')
    plt.ylabel(col)
    plt.tight_layout()
    plt.show()



"""## Multivariate Analysis

### Multivariate Analysis: Numerical Features
"""

corr_matrix = df[numeric_cols].corr()
print("\nCorrelation Matrix (first 5x5 values):\n", corr_matrix.iloc[:5, :5])
plt.figure(figsize=(20, 18))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix of Numerical Features')
plt.show()

"""### Multivariate Analysis: Categorical Features vs. Target Variable ('loan_active')"""

for col in categorical_cols:
    if col != 'loan_active': # Exclude the target itself if it was mistakenly included
        plt.figure(figsize=(10, 6))
        sns.countplot(y=col, hue='loan_active', data=df, order=df[col].value_counts().index[:10], palette='coolwarm')
        plt.title(f'Distribution of {col} by Loan Active Status')
        plt.xlabel('Count')
        plt.ylabel(col)
        plt.legend(title='Loan Active')
        plt.tight_layout()
        plt.show()

# Saving the processed data
output_file_path = "/content/drive/MyDrive/Tangible/DataSet17Dec2025_processed.xlsx"

# Save the DataFrame to an Excel file
df.to_excel(output_file_path, index=False)

print(f"Processed DataFrame saved to: {output_file_path}")

input_file_path = "/content/drive/MyDrive/Tangible/DataSet17Dec2025_processed.xlsx"
df = pd.read_excel(input_file_path)

print(df.shape)
print(df.columns)

y = df['loan_active']
# removing y variable (loan active), date-time variables and account number related variables (We can use data-time variables as well, it is not necessary to remove them)
X = df.drop(columns=['loan_active', 'MAPPED_FORACID', 'MAPPED_CIF_ID', 'ACCT_OPN_DATE', 'CUST_DOB', 'AS_ON'])

"""### Train-Test Split

Split the dataset into training and testing sets (e.g., 80/20 ratio), ensuring stratification on the `y` variable to maintain class distribution in both sets.

"""

from sklearn.model_selection import train_test_split
print("y", y.sum())
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")
print(f"y_train shape: {y_train.sum()}")
print(f"y_test shape: {y_test.sum()}")

X_train.columns

"""## Initial Preprocessing for Model

### Subtask:
Apply one-hot encoding to all categorical features and standard scaling to all numerical features within the X variables. Use `ColumnTransformer` for robust preprocessing, fitting on the training data and transforming both training and testing data.

"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler

# Identify numerical and categorical column names
numerical_cols = X_train.select_dtypes(include=np.number).columns
categorical_cols = X_train.select_dtypes(include='object').columns

# Create a ColumnTransformer for preprocessing
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ],
    remainder='passthrough' # Keep other columns (if any) as they are
)

# Fit the preprocessor on the training data
preprocessor.fit(X_train)

# Transform both training and testing sets
X_train_processed = preprocessor.transform(X_train)
X_test_processed = preprocessor.transform(X_test)

print("Shape of X_train_processed:", X_train_processed.shape)
print("Shape of X_test_processed:", X_test_processed.shape)

# To view the names of the features:
feature_names = preprocessor.get_feature_names_out()
print(feature_names)

"""## Train Initial Logistic Regression Model

Train a logistic regression model using the preprocessed training data.

"""

from sklearn.linear_model import LogisticRegression

# Instantiate the Logistic Regression model
# Using solver='liblinear' or 'saga' for better convergence with scaled data, and increasing max_iter
model = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)

# Fit the model to the preprocessed training data
model.fit(X_train_processed, y_train)

print("Logistic Regression model trained successfully.")

print("Intercept (Bias):", model.intercept_)
print("Coefficients:", model.coef_)
feature_names = preprocessor.get_feature_names_out()
coeffs_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': model.coef_[0]})
coeffs_df = coeffs_df.sort_values(by='Coefficient', key=abs, ascending=False)

print(coeffs_df)

from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc

# Make predictions on the preprocessed test data
y_pred = model.predict(X_test_processed)
y_pred_proba = model.predict_proba(X_test_processed)[:, 1] # Probability of the positive class (class 1)

# Print Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Print Confusion Matrix
print("\nConfusion Matrix:")
conf_matrix = confusion_matrix(y_test, y_pred)
print(conf_matrix)

# Calculate ROC curve and AUC
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--') # Random guess line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity / Recall)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(True, alpha=0.3)
plt.show()


print(f"\nROC AUC Score: {roc_auc:.4f}")

thresholds = [0.2, 0.5, 0.8]
print(f"{'='*60}")
print(f"EVALUATING DIFFERENT THRESHOLDS")
print(f"{'='*60}")

for thresh in thresholds:
    # 3. Create custom predictions based on the current threshold
    # If probability >= threshold, Class is 1. Otherwise, 0.
    y_custom_pred = (y_pred_proba >= thresh).astype(int)

    print(f"\n--- THRESHOLD: {thresh} ---")

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_custom_pred)
    tn, fp, fn, tp = cm.ravel()

    print("Confusion Matrix:")
    print(cm)
    print(f"True Negatives (No loan to non-loan customer): {tn}")
    print(f"False Positives (Loan to non-loan customer):      {fp}")
    print(f"False Negatives (No loan to loan customer):      {fn}")
    print(f"True Positives (Loan to loan customer):           {tp}")

    # Classification Report
    print("\nReport:")
    print(classification_report(y_test, y_custom_pred))
    print("-" * 30)

"""Our model is working really well, as visible from the ROC curve. However, this is not because we have created a very good model. But due to data leakage. We need to remvoe the variables 'FONELOAN_OS' and 'CC_OS' to avoid the data leakage. It, is visible from the weights of the model as well. The weights for features CC_OS and FONELOAN_OS are much more higher then other features. This means that the model is heavily relying on these feaatures."""

file_path = "/content/drive/MyDrive/Tangible/DataSet17Dec2025_processed.xlsx"
df  = pd.read_excel(file_path)

print(f"Processed DataFrame saved to: {file_path}")

y = df['loan_active']
# Removing 'FONELOAN_OS' and 'CC_OS'
# NOTE: Based on your domain knowledge you could remove other fields as well.
X = df.drop(columns=['loan_active','FONELOAN_OS', 'CC_OS', 'MAPPED_FORACID', 'MAPPED_CIF_ID', 'ACCT_OPN_DATE', 'CUST_DOB', 'AS_ON'])

from sklearn.model_selection import train_test_split
print("y", y.sum())
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")
print(f"y_train shape: {y_train.sum()}")
print(f"y_test shape: {y_test.sum()}")

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler

# Identify numerical and categorical column names
numerical_cols = X_train.select_dtypes(include=np.number).columns
categorical_cols = X_train.select_dtypes(include='object').columns

# Create a ColumnTransformer for preprocessing
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ],
    remainder='passthrough' # Keep other columns (if any) as they are
)

# Fit the preprocessor on the training data
preprocessor.fit(X_train)

# Transform both training and testing sets
X_train_processed = preprocessor.transform(X_train)
X_test_processed = preprocessor.transform(X_test)

print("Shape of X_train_processed:", X_train_processed.shape)
print("Shape of X_test_processed:", X_test_processed.shape)

from sklearn.linear_model import LogisticRegression

# Instantiate the Logistic Regression model
model = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)

# Fit the model to the preprocessed training data
model.fit(X_train_processed, y_train)

print("Logistic Regression model trained successfully.")

from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc

# Make predictions on the preprocessed test data
y_pred = model.predict(X_test_processed)
y_pred_proba = model.predict_proba(X_test_processed)[:, 1] # Probability of the positive class (class 1)

# Print Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Print Confusion Matrix
print("\nConfusion Matrix:")
conf_matrix = confusion_matrix(y_test, y_pred)
print(conf_matrix)

# Calculate ROC curve and AUC
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--') # Random guess line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity / Recall)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(True, alpha=0.3)
plt.show()


print(f"\nROC AUC Score: {roc_auc:.4f}")

thresholds = [0.2, 0.5, 0.8]
print(f"{'='*60}")
print(f"EVALUATING DIFFERENT THRESHOLDS")
print(f"{'='*60}")

for thresh in thresholds:
    # 3. Create custom predictions based on the current threshold
    # If probability >= threshold, Class is 1. Otherwise, 0.
    y_custom_pred = (y_pred_proba >= thresh).astype(int)

    print(f"\n--- THRESHOLD: {thresh} ---")

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_custom_pred)
    tn, fp, fn, tp = cm.ravel()

    print("Confusion Matrix:")
    print(cm)
    print(f"True Negatives (No loan to non-loan customer): {tn}")
    print(f"False Positives (Loan to non-loan customer):      {fp}")
    print(f"False Negatives (No loan to loan custmer):      {fn}")
    print(f"True Positives (Loan to loan customer):           {tp}")

    # Classification Report
    print("\nReport:")
    print(classification_report(y_test, y_custom_pred))
    print("-" * 30)

"""### The following are some methods(not all) to handle class imbalances"""

from sklearn.linear_model import LogisticRegression

# Instantiate the Logistic Regression model
# Using solver='liblinear' or 'saga' for better convergence with scaled data, and increasing max_iter
model = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42, class_weight='balanced')

# Fit the model to the preprocessed training data
model.fit(X_train_processed, y_train)

print("Logistic Regression model trained successfully.")

from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc

# Make predictions on the preprocessed test data
y_pred = model.predict(X_test_processed)
y_pred_proba = model.predict_proba(X_test_processed)[:, 1] # Probability of the positive class (class 1)

# Print Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Print Confusion Matrix
print("\nConfusion Matrix:")
conf_matrix = confusion_matrix(y_test, y_pred)
print(conf_matrix)

# Calculate ROC curve and AUC
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--') # Random guess line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity / Recall)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(True, alpha=0.3)
plt.show()


print(f"\nROC AUC Score: {roc_auc:.4f}")

thresholds = [0.2, 0.5, 0.8]
print(f"{'='*60}")
print(f"EVALUATING DIFFERENT THRESHOLDS")
print(f"{'='*60}")

for thresh in thresholds:
    # 3. Create custom predictions based on the current threshold
    # If probability >= threshold, Class is 1. Otherwise, 0.
    y_custom_pred = (y_pred_proba >= thresh).astype(int)

    print(f"\n--- THRESHOLD: {thresh} ---")

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_custom_pred)
    tn, fp, fn, tp = cm.ravel()

    print("Confusion Matrix:")
    print(cm)
    print(f"True Negatives (Correct Rejection): {tn}")
    print(f"False Positives (False Alarm):      {fp}")
    print(f"False Negatives (Missed Oppt):      {fn}")
    print(f"True Positives (Success):           {tp}")

    # Classification Report
    print("\nReport:")
    print(classification_report(y_test, y_custom_pred))
    print("-" * 30)

"""### Apply SMOTE Oversampling

Apply Synthetic Minority Over-sampling Technique (SMOTE) to the training data (`X_train_processed`, `y_train`) to increase the number of instances in the minority class. This will create a more balanced dataset for training.

"""

from imblearn.over_sampling import SMOTE

# Instantiate SMOTE
smote = SMOTE(random_state=42)

# Apply SMOTE to the training data
X_train_smote, y_train_smote = smote.fit_resample(X_train_processed, y_train)

print(f"Original training data shape (X): {X_train_processed.shape}, (y): {y_train.shape}")
print(f"Resampled training data shape (X): {X_train_smote.shape}, (y): {y_train_smote.shape}")
print(f"Class distribution in y_train_smote:\n{pd.Series(y_train_smote).value_counts()}")

from sklearn.linear_model import LogisticRegression


model_smote = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)

# Fit the model to the SMOTE-oversampled training data
model_smote.fit(X_train_smote, y_train_smote)

print("Logistic Regression model trained successfully on SMOTE-oversampled data.")

from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt

# Make predictions on the preprocessed test data using the SMOTE-trained model
y_pred_smote = model_smote.predict(X_test_processed)
y_pred_proba_smote = model_smote.predict_proba(X_test_processed)[:, 1] # Probability of the positive class (class 1)

# Print Classification Report
print("\nClassification Report (SMOTE model):")
print(classification_report(y_test, y_pred_smote))

# Print Confusion Matrix
print("\nConfusion Matrix (SMOTE model):")
conf_matrix_smote = confusion_matrix(y_test, y_pred_smote)
print(conf_matrix_smote)

# Calculate ROC curve and AUC
fpr_smote, tpr_smote, thresholds_smote = roc_curve(y_test, y_pred_proba_smote)
roc_auc_smote = auc(fpr_smote, tpr_smote)

# Plot ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr_smote, tpr_smote, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_smote:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--') # Random guess line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity / Recall)')
plt.title('Receiver Operating Characteristic (ROC) Curve (SMOTE model)')
plt.legend(loc="lower right")
plt.grid(True, alpha=0.3)
plt.show()

print(f"\nROC AUC Score (SMOTE model): {roc_auc_smote:.4f}")

# Evaluate at different thresholds for comparison
thresholds = [0.2, 0.5, 0.8]
print(f"\n{'='*60}")
print(f"EVALUATING DIFFERENT THRESHOLDS FOR SMOTE MODEL")
print(f"{'='*60}")

for thresh in thresholds:
    y_custom_pred_smote = (y_pred_proba_smote >= thresh).astype(int)

    print(f"\n--- THRESHOLD: {thresh} ---")

    cm_smote = confusion_matrix(y_test, y_custom_pred_smote)
    tn, fp, fn, tp = cm_smote.ravel()

    print("Confusion Matrix:")
    print(cm_smote)
    print(f"True Negatives (Correct Rejection): {tn}")
    print(f"False Positives (False Alarm):      {fp}")
    print(f"False Negatives (Missed Oppt):      {fn}")
    print(f"True Positives (Success):           {tp}")

    print("\nReport:")
    print(classification_report(y_test, y_custom_pred_smote))
    print("-" * 30)

"""### Apply Random Under Sampling


Apply Random Under Sampling to the training data (`X_train_processed`, `y_train`) to reduce the number of instances in the majority class. This will create a more balanced dataset for training.

#### Instructions:
1. Import the `RandomUnderSampler` class from `imblearn.under_sampling`.
2. Instantiate `RandomUnderSampler` (you can use `random_state=42` for reproducibility).
3. Apply Random Under Sampling to `X_train_processed` and `y_train` using the `.fit_resample()` method to generate `X_train_rus` and `y_train_rus`.
"""

from imblearn.under_sampling import RandomUnderSampler

# Instantiate RandomUnderSampler
rus = RandomUnderSampler(random_state=42)

# Apply Random Under Sampling to the training data
X_train_rus, y_train_rus = rus.fit_resample(X_train_processed, y_train)

print(f"Original training data shape (X): {X_train_processed.shape}, (y): {y_train.shape}")
print(f"Resampled training data shape (X): {X_train_rus.shape}, (y): {y_train_rus.shape}")
print(f"Class distribution in y_train_rus:\n{pd.Series(y_train_rus).value_counts()}")

from sklearn.linear_model import LogisticRegression

# Instantiate the Logistic Regression model
# Setting solver='liblinear' and increasing max_iter for convergence.
model_rus = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42)

# Fit the model to the Random Under Sampled training data
model_rus.fit(X_train_rus, y_train_rus)

print("Logistic Regression model trained successfully on Random Under Sampled data.")

from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt

# Make predictions on the preprocessed test data using the RUS-trained model
y_pred_rus = model_rus.predict(X_test_processed)
y_pred_proba_rus = model_rus.predict_proba(X_test_processed)[:, 1] # Probability of the positive class (class 1)

# Print Classification Report
print("\nClassification Report (RUS model):")
print(classification_report(y_test, y_pred_rus))

# Print Confusion Matrix
print("\nConfusion Matrix (RUS model):")
conf_matrix_rus = confusion_matrix(y_test, y_pred_rus)
print(conf_matrix_rus)

# Calculate ROC curve and AUC
fpr_rus, tpr_rus, thresholds_rus = roc_curve(y_test, y_pred_proba_rus)
roc_auc_rus = auc(fpr_rus, tpr_rus)

# Plot ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr_rus, tpr_rus, color='darkgreen', lw=2, label=f'ROC curve (AUC = {roc_auc_rus:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--') # Random guess line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity / Recall)')
plt.title('Receiver Operating Characteristic (ROC) Curve (RUS model)')
plt.legend(loc="lower right")
plt.grid(True, alpha=0.3)
plt.show()

print(f"\nROC AUC Score (RUS model): {roc_auc_rus:.4f}")

# Evaluate at different thresholds for comparison
thresholds = [0.2, 0.5, 0.8]
print(f"\n{'='*60}")
print(f"EVALUATING DIFFERENT THRESHOLDS FOR RUS MODEL")
print(f"{'='*60}")

for thresh in thresholds:
    y_custom_pred_rus = (y_pred_proba_rus >= thresh).astype(int)

    print(f"\n--- THRESHOLD: {thresh} ---")

    cm_rus = confusion_matrix(y_test, y_custom_pred_rus)
    tn, fp, fn, tp = cm_rus.ravel()

    print("Confusion Matrix:")
    print(cm_rus)
    print(f"True Negatives (Correct Rejection): {tn}")
    print(f"False Positives (False Alarm):      {fp}")
    print(f"False Negatives (Missed Oppt):      {fn}")
    print(f"True Positives (Success):           {tp}")

    print("\nReport:")
    print(classification_report(y_test, y_custom_pred_rus))
    print("-" * 30)

print("\n" + "="*60)
print("MODEL PERFORMANCE COMPARISON")
print("="*60)

# Extracting Recall and ROC AUC for each model
# Baseline with class_weight='balanced'
# Note: model and y_test values from the previous 'class_weight balanced' run
report_balanced = classification_report(y_test, model.predict(X_test_processed), output_dict=True)
recall_balanced = report_balanced['1']['recall']
roc_auc_balanced = roc_auc

# SMOTE model
report_smote = classification_report(y_test, y_pred_smote, output_dict=True)
recall_smote = report_smote['1']['recall']
roc_auc_smote_val = roc_auc_smote

# RUS model
report_rus = classification_report(y_test, y_pred_rus, output_dict=True)
recall_rus = report_rus['1']['recall']
roc_auc_rus_val = roc_auc_rus

# Print comparison table
print(f"{'Model':<30} | {'Minority Class Recall':<25} | {'ROC AUC Score':<20}")
print(f"{'='*30} | {'='*25} | {'='*20}")
print(f"{'Baseline (class_weight)':<30} | {recall_balanced:<25.4f} | {roc_auc_balanced:<20.4f}")
print(f"{'SMOTE Oversampling':<30} | {recall_smote:<25.4f} | {roc_auc_smote_val:<20.4f}")
print(f"{'RUS Under Sampling':<30} | {recall_rus:<25.4f} | {roc_auc_rus_val:<20.4f}")
print("="*60)

print("\nSummary: Choose the model that best balances minority class recall and overall AUC based on business requirements.")

"""## Introduction to Regularization

### L1 (Lasso) Regularization
L1 regularization, also known as Lasso (Least Absolute Shrinkage and Selection Operator), adds a penalty term equal to the absolute value of the magnitude of the coefficients to the loss function. Its primary effect is to shrink some coefficients to zero, effectively performing feature selection. This makes Lasso useful for models with many features, as it helps identify and remove irrelevant ones.

Mathematically, the penalized loss function for linear regression becomes:
$$ \text{Loss}(W) = \text{MSE}(W) + \lambda \sum_{j=1}^{p} |w_j| $$
where $W$ is the vector of coefficients, $\text{MSE}$ is the Mean Squared Error, and $\lambda$ is the regularization strength (hyperparameter).

### L2 (Ridge) Regularization
L2 regularization, or Ridge regression, adds a penalty term equal to the square of the magnitude of the coefficients to the loss function. This penalty shrinks coefficients towards zero (but rarely to exactly zero), reducing their variance and preventing overfitting without performing explicit feature selection.

Mathematically, the penalized loss function for linear regression becomes:
$$ \text{Loss}(W) = \text{MSE}(W) + \lambda \sum_{j=1}^{p} w_j^2 $$
where $W$ is the vector of coefficients, $\text{MSE}$ is the Mean Squared Error, and $\lambda$ is the regularization strength (hyperparameter).

### Role in Managing Bias-Variance Trade-off
Both L1 and L2 regularization address the bias-variance trade-off:

*   **Reducing Variance (Preventing Overfitting):** When a model is too complex and fits the training data too closely, it tends to have high variance, meaning it performs poorly on unseen data. Regularization adds a penalty for large coefficients, which encourages simpler models. By constraining the magnitude of coefficients, regularization prevents the model from assigning too much importance to any single feature or becoming overly sensitive to noise in the training data. This reduces the model's variance and improves its generalization ability.

*   **Impact on Bias:** While regularization primarily reduces variance, it can slightly increase bias. A simpler model might not capture all the nuances in the training data, leading to a small increase in bias. However, in many real-world scenarios, the reduction in variance achieved through regularization far outweighs the slight increase in bias, leading to a better overall model performance.

*   **Lasso's Specific Benefit:** L1 regularization's ability to drive some coefficients to exactly zero means it can produce sparser models. This is beneficial for interpretability and can help in cases where many features are irrelevant, thus implicitly managing complexity and contributing to a better trade-off by reducing the

 NOTE: Even though L1 regularization acts as an automatic feature selection method. It might still be a good idea to perform explicit feature selection steps to reduce the computation burden on the model. (More feautre means we need more compute to train the model.)

 In sklearn the parameter penalty (in earlier versions or 'l1_ratio' in newer versions can be used to implement/modify/remove regularization)
"""