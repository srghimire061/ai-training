# -*- coding: utf-8 -*-
"""Country Clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_itre3rOgNctMx87Xeexfv62BVmXgf6W

# Country Clustering

## Problem Statement

1. We need to categorise the countries using some socio-economic and health factors that determine the overall development of the country.
2. We then need to suggest the countries which the CEO could focus on the most.

## Data Analysis for the Solution
"""

# importing initial libraries

import numpy as np
import pandas as pd

#import the warnings.

import warnings
warnings.filterwarnings('ignore')

# reading the csv file and storing the information in the variable 'df'

df = pd.read_csv('Country-data.csv')

# getting the first look of the dataframe

df.head()

# checking the shape of the dataframe 'df'

df.shape

"""### 1. Exploratory Data Analysis

#### Chekcking missing values
"""

round (df.isnull().sum()/len(df.index)*100,2)

"""Observation : We do not have any missing values in the coloumns

### 1A. Graphical Univariate Analysis
"""

# importing library for visualization

import matplotlib.pyplot as plt
import seaborn as sns

# creating a tuple that consists of a list of all the columns except 'country' in the dataframe 'df'

feature = df.columns[1:]

# just keeing this her for better understanding - remove later

for i in enumerate (feature):
    print(i)

plt.figure(figsize=(20,10))

# adding number to each feature of the list 'feature' to make use of subplot funcionality using enumerate
# plotting all the variables together to have the idea of how the values in each variables are distributed

for i in enumerate (feature):

    plt.subplot(3,3,i[0]+1)
    sns.distplot(df[i[1]])

plt.show()

"""#### Initial Observations

* The variables 'life_expec', 'total_fer', 'gdpp','income' and 'child_mort' seem to have more predictive power than the other variables based on the shape of the graph and the variance in data we can observe

### 1B. Graphical Bivariate Analysis
"""

S

"""#### Observations

* 'child_mort' is highly correlated with 'life_expec'
* 'total_fer' is also highly correlated with 'child_mort'
*  We can just use 'child_mort' and not loose a significant predictive power of 'life_expec' and 'total_fer'
*  The selection will make clustering easier

#### Final Variables

* Based on the observations from univariate and bivariate analysis, we are selecting the follwing varaibles:
       1. 'child_mort'
       2. 'income'
       3. 'gdpp'

### 1C. Numeric Univarite Analysis and Outlier Treatment

#### Variable One : chid_mort
"""

# 'child_mort' expresses the death of children under the age of 5 per 1000 live births in a specific country

df.child_mort.describe()

# plotting is distplot to understand the variable 'child_mort'

sns.boxplot(df['child_mort'])
plt.show()

# we can notice some outliers

# value at the 90th percentile

df.child_mort.quantile(0.9)

"""We will keep the outliers in the case because there might be countries where the health situation is very bad. These are the kind of countries we are targetting. So the outlier information becomes important in case of this variable and we can not remove them

#### Variable Two : income
"""

# 'income' expresses the net income per person in a specific country

df.income.describe()

# plotting is distplot to understand the variable 'income'

sns.boxplot(df['income'])
plt.show()

# we notice some outiers

# value at the 90th percentile

income_q = df.income.quantile(0.9)

"""The higher values of net income in a country does not add value to our problem statement. Those countries will not fall in the category of countries we can recommend the CEO to focus on."""

# capping the at outliers at 90th percentile

df.income[df.income > 42000] = income_q

# checking the new details of the variable

df.income.describe()

"""#### Variable Three : gdpp"""

# 'gdpp' expresses the GDP per capita in a specific country

df.gdpp.describe()

# plotting is distplot to understand the variable 'gdpp'

sns.boxplot(df['gdpp'])
plt.show()

# we notice some outliers

# value at the 90th percentile

gdpp_q = df.gdpp.quantile(0.9)

"""Similar to net income, the higher values of GDP per capita in a country does not add value to our problem statement. Those countries will not fall in the category of countries we can recommend the CEO to focus on."""

# # capping the at outliers at 90th percentile

df.gdpp[df.gdpp > 27000] = gdpp_q

# checking the new details of the variable

df.gdpp.describe()

"""## 2. Hopkins Score

#### Checking the Hopkins Score
"""

# importing the relevant packages

from sklearn.neighbors import NearestNeighbors
from random import sample
from numpy.random import uniform
import numpy as np
from math import isnan

# Calculating the Hopkins score by using an external library

def hopkins(X):
    d = X.shape[1]
    n = len(X)
    m = int(0.1 * n)
    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)

    rand_X = sample(range(0, n, 1), m)

    ujd = []
    wjd = []
    for j in range(0, m):
        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)
        ujd.append(u_dist[0][1])
        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)
        wjd.append(w_dist[0][1])

    H = sum(ujd) / (sum(ujd) + sum(wjd))
    if isnan(H):
        print(ujd, wjd)
        H = 0

    return H

# creating a new dataframe with only the selected columns for clustering

df1 = df.drop(['country'], axis = 1)

df1.head()

# Calcualting the hopkins score multiple times to check for the different values that the hopkins score might take

(for i in range (1,11):
    print(hopkins(df1)))

"""The Hopkins score is greater that 80 percentage even after runining it more than 10 times. So we can understand that the data has good clustering tendency

## 3. Scaling
"""

# importing relevant library

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

df2 = scaler.fit_transform(df1)


df2 = pd.DataFrame(df2, columns = df1.columns[:])

df2.head()

"""## 4. K Means CLustering

#### Choosing the value of K with Silhouttee Score
"""

# importing revelant libraries

from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans

ss = []
for k in range(2, 11):
    kmean = KMeans(n_clusters = k).fit(df2)
    ss.append([k, silhouette_score(df2, kmean.labels_)])

temp = pd.DataFrame(ss)
plt.plot(temp[0], temp[1])
plt.show()

"""#### Note :
1. Though the peak value of silhouttee score is in between 3 and 4
2. We will check the Elbow curve before making a conclusion on the number of clusters

#### Choosing the value of K with Elbow curve
"""

ssd = []
for k in range(2, 11):
    kmean = KMeans(n_clusters = k).fit(df2)
    ssd.append([k, kmean.inertia_])

temp = pd.DataFrame(ssd)
plt.plot(temp[0], temp[1])
plt.show()

"""Note:
1. In this curve we have a break point at 3,4 and 6
2. In elbow curve we prefer choosing the lesser clusters in case of multiple bends

Note:

We choose the value of 3 and 4 based on both the plots

### 4A. Clustering : Creating 3 different clusters
"""

# creating 3 different clusters

kmean = KMeans(n_clusters = 3, random_state = 100)
kmean.fit(df2)

# creating the dataframe from the output we got from K Mean algorithm

cluster_3  = pd.DataFrame(kmean.labels_, columns= ['cluster_3'])
cluster_3.head()

# concatinating the dataframe :  clusters_1 and kmean

df_kmean = pd.concat([df, cluster_3], axis =1)
df_kmean.head()

# checking the value counts in each cluster

df_kmean.cluster_3.value_counts()

"""### 4B. Cluster Profiling based on K Means Clustering , k=3"""

plt.figure(figsize = [20,5])

plt.subplot(1,3,1)
sns.boxplot(x= 'cluster_3', y ='child_mort', data = df_kmean)

plt.subplot(1,3,2)
sns.boxplot(x= 'cluster_3', y ='income', data = df_kmean)

plt.subplot(1,3,3)
sns.boxplot(x= 'cluster_3', y ='gdpp', data = df_kmean)

plt.show()

"""#### Properties of Different Clusters

1. Cluster 2 has very high child mortality, low net income per person and very low GDP per capita
2. Cluster 1 has medium range child mortality , medium range net income per person and low GDP per capita
3. Cluster 0 has low child mortality , high net income per person and high GDP per capita
"""

# Note : The cluster of our interest is cluster 2

"""### 4C. Focus Countries based on K Mean Clustering , k=3"""

df_kmean[df_kmean['cluster_3'] == 2].shape

# We have 46 countries that fall in this category

# countries that fall into this category

df_kmean[df_kmean['cluster_3'] == 2].country

# sorting the countries to find the top 5 countries to focus on

df_kmean[df_kmean['cluster_3'] == 2].sort_values(by = ['child_mort','income','gdpp'] , ascending = [False, True, True]).head(5)

"""#### The countries we can recommend the CEO to focus on based on K Means clustering as k = 3 are

1. Haiti
2. Sierra Leone
3. Chad
4. Central African Republic
5. Mali

### 4D. Creating 4 different clusters
"""

# creating 4 different clusters

kmean = KMeans(n_clusters = 4, random_state = 100)
kmean.fit(df2)

# creating the dataframe from the output we got from K Mean algorithm

cluster_4  = pd.DataFrame(kmean.labels_, columns= ['cluster_4'])
cluster_4.head()

# concatinating the dataframe :  clusters and kmean

df_kmean = pd.concat([df_kmean, cluster_4], axis =1)
df_kmean.head()

# checking the value counts in each cluster

df_kmean.cluster_4.value_counts()

"""### 4E. Cluster Profiling based on K Means Clustering, k=4"""

plt.figure(figsize = [20,5])

plt.subplot(1,3,1)
sns.boxplot(x= 'cluster_4', y ='child_mort', data = df_kmean)

plt.subplot(1,3,2)
sns.boxplot(x= 'cluster_4', y ='income', data = df_kmean)

plt.subplot(1,3,3)
sns.boxplot(x= 'cluster_4', y ='gdpp', data = df_kmean)

plt.show()

"""1. Cluster 0 has low child mortality, low net income per person and low GDP per capita
2. Cluster 1 has low range child mortality , high net income per person and high GDP per capita
3. Cluster 2 has high child mortality , very low net income per person and very low GDP per capita
4. Cluster 3 has low child mortality , high net income per person and high GDP per capita
"""

# Note : cluster of interest is cluster 2

"""### 4F. Focus Countries based on K Mean Clustering , k=4"""

df_kmean[df_kmean['cluster_4'] == 2].country

# sorting the countries to find the top 5 countries to focus on

df_kmean[df_kmean['cluster_4'] == 2].sort_values(by = ['child_mort','income','gdpp'] , ascending = [False, True, True]).head(5)

"""#### The countries we can recommend the CEO to focus on based on K Means clustering as k = 4 are

1. Haiti
2. Sierra Leone
3. Chad
4. Central African Republic
5. Mali

#### Note

We used the scaled dataframe with all the continuous columns in case of K means clustering because it was giving decent clusters. We have not done the same with Hierarchical Clustering as proper clusters were not forming. We have only used the selected variables for Hierarchical Clustering.

## 5. Hierarchical Clustering
"""

# importing relevant libraries

from scipy.cluster.hierarchy import linkage
from scipy.cluster.hierarchy import dendrogram
from scipy.cluster.hierarchy import cut_tree

# checking the scaled dataframe : This is the same dataframe we used in the K Means clustering

df2.head()

#getting the dataframe with the countries and the clusters derived from K Means Clustering

df_kmean.head()

# creating a new dataframe df_h for hierarchial clustering

df_h = df2.drop(['exports','health','imports','inflation','life_expec','total_fer'] , axis =1)

# Observing the new dataframe

df_h.head()

"""#### Single Linkage"""

# Checking the clustering based on single linkage

output = linkage(df_h, method ='single',metric = 'euclidean')

dendrogram(output)
plt.show()

"""1. It is very difficult to read the results based on single linkage
2. The formation of clusters do not seem to be appropriate

#### Complete linkage
"""

output = linkage(df_h, method ='complete',metric = 'euclidean')

dendrogram(output)
plt.show()

"""1. It is easier to read the result based on complete linkage
2. The formation of clusters seem appropriate

#### Choosing the number of clusters
"""

# We are choosing to create 3 clusters based in reference to the above dendrogram

cut_tree(output, n_clusters = 3).shape

# creating the array clusters_h

clusters_h = cut_tree(output, n_clusters = 3).reshape(-1,)

# Assigning a new column - 'clusters_h' to the df_kmean dataframe

df_kmean['clusters_h']= clusters_h

# Obaserivng the df_kmean dataframe

df_kmean.head()

"""#### Ploting the clusters"""

plt.figure(figsize = [20,5])

plt.subplot(1,3,1)
sns.boxplot(x= 'clusters_h', y ='child_mort', data = df_kmean)

plt.subplot(1,3,2)
sns.boxplot(x= 'clusters_h', y ='income', data = df_kmean)

plt.subplot(1,3,3)
sns.boxplot(x= 'clusters_h', y ='gdpp', data = df_kmean)

plt.show()

"""1. Cluster 0 has medium range child mortality , medium range net income per person and medium GDP per capita
2. Cluster 1 has very high child mortality, very low net income per person and very low GDP per capita
3. Cluster 2 has low child mortality , high net income per person and high GDP per capita
"""

# Note : The cluster of our interest is cluster 1

# checking the countries in cluster 1

df_kmean[df_kmean['clusters_h'] == 1]

# sorting the countries to find the top 5 countries to focus on

df_kmean[df_kmean['clusters_h'] == 1].sort_values(by = ['child_mort','income','gdpp'] , ascending = [False, True, True]).head(5)

"""#### The countries we can recommend the CEO to focus on based on Hierarchial clustering are

1. Haiti
2. Sierra Leone
3. Chad
4. Central African Republic
5. Mali

## 6. Final Recommendation

We found the same focus countries based on both K Means clutsering and Hierarchial Clustering.

#### The focus countires

1. Haiti
2. Sierra Leone
3. Chad
4. Central African Republic
5. Mali
"""