# -*- coding: utf-8 -*-
"""Regression Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14loYivzYmzGOuOBC1VY8KnEHxiaSedRL

# Lead Scoring Study

## A. Problem Statment

1. Building a logistic regression model to assign a lead score between 0 and 100 to each of the leads which can be used by the company to target potential leads
2. Creating a model that can adjust according to company's requirements
    - When sales team is recruited and the team wants to contact all the leads that has good chances of conversion
    - When the sales team is involved in other project and will only call when there is a requirement

#### Importing Revelant Libraries
"""

import numpy as np
import pandas as pd

import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('Leads.csv')

# checking the shape of dataframe

df.shape

# observing the dataframe for the first time

df.head()

"""## B. Data Preperation and EDA"""

# checking for columns with 'select' field

df.columns[df.isin(['Select']).any()]

# replacing 'Select' by nan in 'Specialization'

df['Specialization'] = df['Specialization'].replace(np.nan, 'Select')

# replacing 'Select' by nan in 'How did you hear about X Education'

df['How did you hear about X Education'] = df['How did you hear about X Education'].replace(np.nan, 'Select')

# replacing 'Select' by nan in 'Lead Profile'

df['Lead Profile'] = df['Lead Profile'].replace(np.nan, 'Select')

#replacing 'Select' by nan in 'City'

df['City'] = df['City'].replace(np.nan, 'Select')

# checking the percentage of missing values

round(df.isnull().sum()/len(df),2)

"""#### Dropping rows with missing high percentage of missing values

- 'Lead Quality' is a metric based on the intution of the employee who has been assigned the lead. With high missing values it does not hold significance to the model we will create
"""

# droppping 'Lead Quality'

df.drop('Lead Quality' , axis =1 , inplace = True)

"""#### Note

The following are metrics assigned to each customer based on their activity and profile. It does not hold high significance to understand their state with hot leads specially when they have high missing values
1. 'Asymmetrique Activity Index'
2. 'Asymmetrique Profile Index'
3. 'Asymmetrique Activity Score'
4. 'Asymmetrique Profile Score'
"""

# dropping the above mentioned rows

df.drop('Asymmetrique Activity Index' , axis =1 , inplace = True)

df.drop('Asymmetrique Profile Index' , axis =1 , inplace = True)

df.drop('Asymmetrique Activity Score' , axis =1 , inplace = True)

df.drop('Asymmetrique Profile Score' , axis =1 , inplace = True)

"""#### Note

- 'Country' has significant number of missing values and is not a important variable for the problem statement we have
"""

# dropping 'Country'

df.drop('Country' , axis =1 , inplace = True)

# checking the percentage of missing values

round(df.isnull().sum()/len(df),2)

"""#### Note

- 'What is your current occupation' , 'What matters most to you in choosing a course' and 'Tags' have might high number of missing values but they have important information for the problem statement. Doing some analysis on the same
"""

df['What is your current occupation'].value_counts()

"""#### Note

- The majority of information in 'What is your current occupation' does not have enough significance in the categorisation of hot leads. Between unemployed, workig professional and student it has most of the information. With high missing values this column is not significant enough. Thus dropping 'What is your current occupation'
"""

# dropping 'What is your current occupation'

df.drop('What is your current occupation' , axis =1 , inplace = True)

df['What matters most to you in choosing a course'].value_counts()

"""#### Note

- 'What matters most to you in choosing a course' has most of its information as Better Career Prospects. Thus if we drop the column it will not make a lot of difference to the model
"""

# dropping 'What matters most to you in choosing a course'

df.drop('What matters most to you in choosing a course' , axis =1 , inplace = True)

df['Tags'].value_counts()

"""#### Note

- The information with large values in 'Tags' such as -Will revert after reading the email- have less significance in making with regards to our problem statement. We can drop this column
"""

# dropping 'Tags'

df.drop('Tags' , axis =1 , inplace = True)

# checking the missing values now

round(df.isnull().sum()/len(df.index),2)

"""#### Note

- We do not have significant missing values in the dataframe now. We can proceed to the next step.

#### Finding highly skewed columns

- Skewed columns are columns that have categorical values but are highly polarised in terms of their data
"""

df.columns

# dropping 'Do Not Call'

df.drop('Do Not Call', axis =1 , inplace = True)

df['Magazine'].value_counts()

# dropping 'Magazine'

df.drop('Magazine', axis=1 , inplace = True)

df['Search'].value_counts()

# dropping 'Search'

df.drop('Search' , axis =1, inplace =True)

df['Newspaper Article'].value_counts()

# dropping 'Newspaper Article'

df.drop('Newspaper Article', axis =1 , inplace = True)

df['X Education Forums'].value_counts()

# dropping 'X Education Forums'

df.drop('X Education Forums', axis =1 , inplace = True)

df['Newspaper'].value_counts()

# dropping 'Newspaper'

df.drop('Newspaper', axis=1 , inplace = True)

df['Digital Advertisement'].value_counts()

# dropping 'Digital Advertisement'

df.drop('Digital Advertisement', axis=1 , inplace = True)

df['Through Recommendations'].value_counts()

# dropping 'Through Recommendations'

df.drop('Through Recommendations', axis=1 , inplace = True)

df['Receive More Updates About Our Courses'].value_counts()

# dropping 'Receive More Updates About Our Courses'

df.drop('Receive More Updates About Our Courses', axis=1 , inplace = True)

df['Update me on Supply Chain Content'].value_counts()

# dropping 'Update me on Supply Chain Content'

df.drop('Update me on Supply Chain Content', axis=1 , inplace = True)

df['Get updates on DM Content'].value_counts()

# dropping 'Get updates on DM Content'

df.drop('Get updates on DM Content', axis=1 , inplace = True)

df['I agree to pay the amount through cheque'].value_counts()

# dropping 'I agree to pay the amount through cheque'

df.drop('I agree to pay the amount through cheque', axis=1 , inplace = True)

"""#### Note

- The other columns do not have skewed values in the dataframe and we can proceed to the next step

#### Checking for categorical columns with less percentage of rows
"""

df.columns

df['Lead Source'].value_counts()

# putting the values with less than 100 occurences in one category - other

lis_ls = ['Facebook','bing','google','Click2call','Live Chat','Social Media','Press_Release','blog','Pay per Click Ads','welearnblog_Home','WeLearn','testone','NC_EDM','youtubechannel']

df['Lead Source'] = df['Lead Source'].apply(lambda x : 'Other_ls' if x in lis_ls else x)

df['Lead Source'].value_counts()

df['How did you hear about X Education'].value_counts()

# putting the values with less than 100 occurences in one category - other

lis_ed = ['Advertisements','Social Media','Email','SMS']

df['How did you hear about X Education'] = df['How did you hear about X Education'].apply(lambda x : 'Other_ed' if x in lis_ed else x)

df['How did you hear about X Education'].value_counts()

df['Lead Profile'].value_counts()

lis_lp = ['Lateral Student','Dual Specialization Student']

df['Lead Profile'] = df['Lead Profile'].apply(lambda x : 'Other_lp' if x in lis_lp else x)

df['Lead Profile'].value_counts()

df['Last Activity'].value_counts()

lis_la = ['Unreachable','Unsubscribed','Had a Phone Conversation','View in browser link Clicked','Approached upfront','Email Marked Spam','Email Received','Resubscribed to emails','Visited Booth in Tradeshow']

df['Last Activity'] = df['Last Activity'].apply(lambda x : 'Other_la' if x in lis_la else x)

df['Last Activity'].value_counts()

df['Last Notable Activity'].value_counts()

lis_na = ['Email Bounced','Unsubscribed','Unreachable','Had a Phone Conversation','Email Marked Spam','Resubscribed to emails','Email Received','Form Submitted on Website','View in browser link Clicked','Approached upfront']

df['Last Notable Activity'] = df['Last Notable Activity'].apply(lambda x : 'Other_na' if x in lis_na else x)

df['Last Notable Activity'].value_counts()

round(df.isnull().sum()/len(df.index),2)

"""#### Removing any remaining null values"""

round(df.isnull().sum()/len(df.index),2)

# dropping the null values in the remaiaining columns as their number is very small

df.dropna(inplace = True)

# checking for the null variables

round(df.isnull().sum()/len(df.index),2)

#### Checking the percentage of the rows that are left

a = len(df.index)/9240
print(a)

# We have retainied more than 98 percentage of the rows after removing the null values

"""#### Creating Dummies for all the categorical variables"""

df.info()

"""#### Variable One : Lead Origin"""

leadorigin_dummy = pd.get_dummies(df['Lead Origin'], drop_first = True, dtype = int)
leadorigin_dummy.head()

"""#### Variable Two : Lead Source"""

leadsource_dummy = pd.get_dummies(df['Lead Source'], drop_first = True, dtype = int)
leadsource_dummy.head()

"""#### Variable Three : Do Not Email"""

# creating a function to clearly mention the yes and its related variable

def email(x):
    if x == 'Yes':
        return 'yes_email'
    else:
        return 'no_email'

df['Do Not Email'] = df['Do Not Email'].apply(email)

donotemail_dummy = pd.get_dummies(df['Do Not Email'], drop_first = True, dtype = int)
donotemail_dummy.head()

"""#### Variable Four : Last Activity"""

def sms(x):
    if x == 'SMS Sent':
        return 'sms_sent_la'
    if x == 'Olark Chat Conversation':
        return 'Olark_Chat_Conversation_la'
    else:
        return x

df['Last Activity'] = df['Last Activity'].apply(sms)

df['Last Activity'].value_counts()

lastactivity_dummy = pd.get_dummies(df['Last Activity'], drop_first = True, dtype = int)
lastactivity_dummy.head()

"""#### Variable Five : Specialization"""

df['Specialization'].value_counts()

def specs(x):
    if x == 'Select':
        return 'select_specs'
    else:
        return x

df['Specialization'] = df['Specialization'].apply(specs)

specialization_dummy = pd.get_dummies(df['Specialization'], drop_first = True, dtype = int)
specialization_dummy.head()

"""#### Variable Six : How did you hear about X Education"""

df['How did you hear about X Education'].value_counts()

def edu (x):
    if x == 'Student of SomeSchool':
        return 'edu_someschool'
    else:
        return x

df['How did you hear about X Education'] = df['How did you hear about X Education'].apply(edu)

education_dummy = pd.get_dummies(df['How did you hear about X Education'], drop_first = True, dtype = int)
education_dummy.head()

"""#### Variable Seven : Lead Profile"""

df['Lead Profile'].value_counts()

def lp(x):
    if x == 'Select':
        return 'select_lp'
    elif x == 'Student of SomeSchool':
        return 'lp_someschool'
    else:
        return x

df['Lead Profile'] = df['Lead Profile'].apply(lp)

leadprofile_dummy = pd.get_dummies(df['Lead Profile'], drop_first = True, dtype = int)
leadprofile_dummy.head()

"""#### Variable Eight : City"""

df['City'].value_counts()

def city (x):
    if  x == 'Select':
        return 'select_city'
    else:
        return x

df['City'] = df['City'].apply(city)

city_dummy = pd.get_dummies(df['City'], drop_first = True, dtype = int)
city_dummy.head()

"""#### Variable Nine : A free copy of Mastering The Interview"""

df['A free copy of Mastering The Interview'].value_counts()

# creating a function to clearly mention the yes and its related variable

def emails(x):
    if x == 'Yes':
        return 'yes_copy'
    else:
        return 'no_copy'

df['A free copy of Mastering The Interview'] = df['A free copy of Mastering The Interview'].apply(emails)

copy_dummy = pd.get_dummies(df['A free copy of Mastering The Interview'], drop_first = True, dtype = int)
copy_dummy.head()

final_train.info()

"""#### Variable Ten : Last Notable Activity"""

df['Last Notable Activity'].value_counts()

activity_dummy = pd.get_dummies(df['Last Notable Activity'], drop_first = True, dtype = int)
activity_dummy.head()

#### Putting all the dummy dataframe together

final = pd.concat([df,leadorigin_dummy,leadsource_dummy,donotemail_dummy,lastactivity_dummy,specialization_dummy,education_dummy,leadprofile_dummy,city_dummy,copy_dummy,activity_dummy],axis =1)
final.head()

# dropping all the related variables after creating the dummies

final = final.drop('Lead Origin',axis =1)
final = final.drop('Lead Source',axis =1)
final = final.drop('Do Not Email',axis =1)
final = final.drop('Last Activity',axis =1)
final = final.drop('Specialization',axis =1)
final = final.drop('How did you hear about X Education',axis =1)
final = final.drop('Lead Profile',axis =1)
final = final.drop('City',axis =1)
final = final.drop('A free copy of Mastering The Interview',axis =1)
final = final.drop('Last Notable Activity',axis =1)

final.head()

"""## C. Dividing into train test data"""

# importing relevant libraries

import sklearn
from sklearn.model_selection import train_test_split

final_train, final_test = train_test_split(final, train_size = 0.7, random_state = 100)

print(final_train.shape)
print(final_test.shape)

"""## D. Performing scaling on continuous variables"""

# we are using the min max scaler for rescaling

#importing relevant packages

from sklearn.preprocessing import MinMaxScaler

# Instiating scaler object

scaler = MinMaxScaler()

# creating a list of numeric variables

numeric = ['TotalVisits','Total Time Spent on Website','Page Views Per Visit']

final_train[numeric] = scaler.fit_transform(final_train[numeric])

final_train.head()

"""## E. Creating the test train split"""

from sklearn.model_selection import train_test_split

x = final.drop(['Prospect ID','Lead Number','Converted'], axis =1)

x.head()

y = final['Converted']

y.head()

x_train, x_test, y_train, y_test = train_test_split(x,y, train_size = 0.7, test_size = 0.3, random_state = 100)

# calculating the conversion rate

conversion = sum(final['Converted'])/len(final['Converted'].index)
conversion

"""- We have a converstion rate of 37.86 percentage

## F. Building the first model
"""

import statsmodels.api as sm

model = sm.GLM(y_train, (sm.add_constant(x_train)), family = sm.families.Binomial())
model.fit().summary()

"""## G. Selecting the Variables with RFE"""

# importing relevant libraries

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()

# running Recursive Feature Elimination (RFE)

from sklearn.feature_selection import RFE
rfe = RFE(estimator=logreg, n_features_to_select = 10)

rfe = rfe.fit(x_train, y_train)

# creating a list of all the variables and their RFE rankings

list(zip(x_train.columns, rfe.support_, rfe.ranking_))

# storing the selected columns in col

col = x_train.columns[rfe.support_]

x_train[col].head()

# chekcking the selected columns

x_train.columns[rfe.support_]

"""## H. Building the Model Based on the selected variables"""

# creating the model

x_train_sm = sm.add_constant(x_train[col])
model_one = sm.GLM(y_train, x_train_sm, family = sm.families.Binomial())
model_one.fit().summary()

# importing relevant libraries

from statsmodels.stats.outliers_influence import variance_inflation_factor

# creating a new dataframe that consists of the name of the variables and their respective VIF

vif = pd.DataFrame()

# assigning the column 'variables' with the columns of X_train

vif['variables'] = x_train[col].columns

# assigning the column 'VIF' with the VIF values of the variales

vif['VIF'] = [variance_inflation_factor(x_train[col].values,i) for i in range (x_train[col].shape[1])]

# rounding off the VIF values to 2 decimal places

vif['VIF'] = round(vif['VIF'],2)

# sorting the VIF values in descending order

vif = vif.sort_values(by = 'VIF' , ascending = False)

# printing the dataframe vif

vif

# 'other_lp' has a very low significance based on the p-value

new = x_train[col]

new.drop('Other_lp', axis = 1, inplace = True)

# creating the model after dropping 'Other_lp'

x_train_sm = sm.add_constant(new)
model_one = sm.GLM(y_train, x_train_sm, family = sm.families.Binomial())
pred = model_one.fit()
pred.summary()

# creating a new dataframe that consists of the name of the variables and their respective VIF

vif = pd.DataFrame()

# assigning the column 'variables' with the columns of X_train

vif['variables'] = new.columns

# assigning the column 'VIF' with the VIF values of the variales

vif['VIF'] = [variance_inflation_factor(new.values,i) for i in range (new.shape[1])]

# rounding off the VIF values to 2 decimal places

vif['VIF'] = round(vif['VIF'],2)

# sorting the VIF values in descending order

vif = vif.sort_values(by = 'VIF' , ascending = False)

# printing the dataframe vif

vif

# Getting the predicted values on the train set

y_train_pred = pred.predict(x_train_sm)
y_train_pred[:10]

y_train_pred = y_train_pred.values.reshape(-1)
y_train_pred[:10]

# creating the columns 'Convert' and 'Convert_prob' for the dataframe y_train_final

y_train_final = pd.DataFrame({'Convert' :y_train.values, 'Convert_prob' : y_train_pred })

# creating a column 'Lead_Number' for hte datframe y_train_final

y_train_final['Lead_Number'] = final['Lead Number']

y_train_final.head()

# creating a column predicted with the cutoff at 0.5

y_train_final['predicted'] = y_train_final.Convert_prob.map(lambda x: 1 if x >0.5 else 0)

y_train_final.head()

# calculating the score and storing the information in the column 'score'

y_train_final['score'] = round(y_train_final['Convert_prob']*100,0)

y_train_final.head()

"""## Solution One

- The score in the 'score' column of y_train_final is the value we need as the first part of the solution for the problem

## I. Evaluating the Model

#### Checking the accuracy of the model
"""

# importing relevant libraries

from sklearn import metrics

# creating the confusion matrix

confusion = metrics.confusion_matrix(y_train_final.Convert, y_train_final.predicted)
print(confusion)

# Predicted     not-churn    churn
# Actual
# not-churn       3323        582
# churn            735       1711

print(metrics.accuracy_score(y_train_final.Convert, y_train_final.predicted))

"""#### Checking the further metrices"""

# true positives
tp = confusion[1,1]

#true negatives
tn = confusion[0,0]

# false positives
fp = confusion[0,1]

# false negatives
fn = confusion[1,0]

# sensitivity of the model : A measure of how accurately does the model predict the positive outcomes
# sesitivity can also be termed as true positive rate

tp /(tp+fn)

"""The model has low sensitivity. For our requirement this metric needs to be high."""

# specificity of the model : A measure of how accurately does the model predict negative outcomes

tn /(tn+fp)

# false positive rate : A measure of how many 0s were predicted as 1

fp /(tn+fp)

"""The model has high specificity

#### Checking the ROC curve
"""

# importing relevant libraries

import matplotlib.pyplot as plt

# preparing the ROC curve

def draw_roc( actual, probs ):
    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,
                                              drop_intermediate = False )
    auc_score = metrics.roc_auc_score( actual, probs )
    plt.figure(figsize=(5, 5))
    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic example')
    plt.legend(loc="lower right")
    plt.show()

    return None

# Calling the function
draw_roc(y_train_final.Convert, y_train_final.Convert_prob)

"""#### Note

- The area under the ROC curve is 0.83 which depicts that the model is a good classifier
"""

# checking different probablity cutoffs

numbers = [float(x)/10 for x in range(10)]
for i in numbers:
    y_train_final[i] = y_train_final.Convert_prob.map(lambda x:1 if x>i else 0)
y_train_final.head()

# creating a dataframe with information about probability, accuracy, sensitivity and specificity

cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])
#from sklearn.metrics import confusion_matrix

# tp = confusion[1,1]  true positive
# tn = confusion[0,0]  true negatives
# fp = confusion[0,1]  false positives
# fn = confusion[1,0]  false negatives

num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]
for i in num:
    cm1 = metrics.confusion_matrix(y_train_final.Convert, y_train_final[i])
    total1=sum(sum(cm1))
    accuracy = (cm1[0,0]+cm1[1,1])/total1

    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])
    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])

    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]

print(cutoff_df)

# If we have to be extremely sure : we can choose 0.2 because sensitivity is 97 percentage and accuracy 53 percentage
# If we have to save time then : we can choose 0.5 because accuracy : 0.79 sensi: 0.70

# plotting probability

cutoff_df.plot.line(x= 'prob', y = ['accuracy','sensi','speci'])
plt.show()

"""#### Note

- According to the graph the optimal value of cutoff seems to be around 0.3

## Solution Two

### Case One : When we hire summer interns
"""

y_train_final['predicted_interns'] = y_train_final.Convert_prob.map(lambda x: 1 if x >0.2 else 0)

y_train_final['predicted_interns'].value_counts()

# When we hire summer interns we can call 5290 people based on the cutoff of 0.2

"""#### Financial Note

When we mark the model cutoff at 0.2, the sales team has 83 percent people (from the train set its 5290/6351) to call during this period. The accuracy of the Model is not the greatest. However, we will not miss people who could get converted as the sensitivity of the model at this cutoff is very high.

This allows us to not miss any customer who can get converted. Thus for the company this makes more financial sense during this period as:
1. We are completely focused on sales
2. We have hired interns to do this job so the cost for result is very low
3. We will be able to convert more customers hence more revenue

### Case Two : When we make call with necessary
"""

y_train_final['predicted_necessary'] = y_train_final.Convert_prob.map(lambda x: 1 if x >0.5 else 0)

y_train_final['predicted_necessary'].value_counts()

# When we calling on necessity we call 2293 people based on the cutoff of 0.5

"""#### Financial Note

When we can mark the model cutoff at 0.5, the sales team has 36 percent people (from the train set its 2293/6351) to call during this period. This percentage is very close to the actual conversion rate which is 37.85 percentage. The accuracy at this cutoff is almost the highest, the sensitivity is close to maximum and we will end up calling the most optimal set of customers.

This allows us to only focus on the customers that are more likely to convert. Thus for the company this makes more financial sense during period as:
1. We have other things to focus on, so we wont be able to dedicate a lot of resources to calling
2. We can make optimal use of our human resource as we have lesser customers to target who are more likely to convert
3. The sales team is able to focus on other work of higher importance

## J. Making Predictions

#### Scaling the continuous features
"""

# tarnsforming the continuous features in the test dataset

final_test[numeric] = scaler.fit_transform(final_test[numeric])

final_test.head()

# using the features in the test dataframe that were used to create our model

x_test = x_test[col]

x_test.head()

# dropping the feature that our model had shown to have very less significance

x_test.drop('Other_lp', axis=1,inplace =True )

# adding the constant to x_test

x_test_sm = sm.add_constant(x_test)

# creating the predictions based on the model

y_test_pred = pred.predict(x_test_sm)
y_test_pred[:10]

# creating different columns to store the covert situation and the probability for the same

y_test_final = pd.DataFrame({'Convert' : y_test.values, 'Convert_prob' : y_test_pred})

y_test_final.head()

# testing with a cutoff at 0.3 . We can use different cutoffs as required

y_test_final['predicted'] = y_test_final.Convert_prob.map(lambda x: 1 if x >0.5 else 0)

# creating the confusion matrix

confusion_one = metrics.confusion_matrix(y_test_final.Convert, y_test_final.predicted)
print(confusion_one)

# Predicted     not-churn    churn
# Actual
# not-churn       1487        247
# churn            322        667

# the accuracy of the model

print(metrics.accuracy_score(y_test_final.Convert, y_test_final.predicted))

# true positives
tp1 = confusion_one[1,1]

#true negatives
tn1 = confusion_one[0,0]

# false positives
fp1 = confusion_one[0,1]

# false negatives
fn1 = confusion_one[1,0]

# sensitivity of the model : A measure of how accurately does the model predict the positive outcomes
# sesitivity can also be termed as true positive rate

tp1 /(tp1+fn1)

# specificity of the model : A measure of how accurately does the model predict negative outcomes

tn1 /(tn1+fp1)

# false positive rate : A measure of how many 0s were predicted as 1

fp1 /(tn1+fp1)

"""#### Note

1. Model works well
2. The accuracy, sensitivity, specificity are very close to the one generated by the train set on a similar cutoff
3. We can select different cutoff to increase the accuracy, sensitivity and specificity based on the requirement of the case in context
"""