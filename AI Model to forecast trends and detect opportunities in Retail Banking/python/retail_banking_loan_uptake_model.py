# -*- coding: utf-8 -*-
"""Retail_Banking_Loan_Uptake_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rTLinox6WpOzeKlBoRVdt59dvgqp1e97

# 1. Notebook Setup & Data Loading

1.1 Environment Setup
"""

# Core libraries
import pandas as pd
import numpy as np

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Display settings
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 200)

print("Environment ready")

"""1.2 Upload Dataset

"""

from google.colab import files

uploaded = files.upload()

"""1.3 Load the Data"""

# Load main dataset
df = pd.read_excel("DataSet17Dec2025.xlsx")

# Load field description
field_desc = pd.read_excel("FieldDesc.xlsx")

print("Main Dataset Shape:", df.shape)
print("Field Description Shape:", field_desc.shape)

"""1.4 First Look at Data"""

# Preview first 5 rows
df.head()

"""1.5 Column Overview"""

# Column names and data types
df.info()

"""1.6 Field Description Preview"""

field_desc.head(10)

"""# 2. Dataset Understanding & Feature Categorization

2.1 Create Feature Groups (Manual + Explicit)
"""

# ===============================
# Feature Categorization
# ===============================

identifier_cols = [
    'MAPPED_FORACID',
    'MAPPED_CIF_ID',
    'MAPPED_OP_AC'
]

date_cols = [
    'ACCT_OPN_DATE',
    'CUST_DOB',
    'AS_ON'
]

numerical_cols = [
    'SANCT_LIM',
    'INTTRATE',
    'OVD_AMT',
    'MONTHLY_AVG_DEP',
    'FD_AMT',
    'MONTHLY_AVG_CR_AMT',
    'MONTHLY_AVG_DR_AMT',
    'MONTHLY_NO_OF_TXNS',
    'NO_OF_QR_TXNS',
    'NO_OF_POS_TXNS',
    'FONELOAN_OS',
    'CC_OS',
    'LOAN_TENURE_MONTHS'
]

categorical_cols = [
    'SCHM_TYPE',
    'COLLATERAL_TYPE',
    'REP_FREQ',
    'PREPAYMENT_HIST',
    'DISTRICT',
    'PROVINCE',
    'PROFESSION',
    'RISK_RATING',
    'DORMANT_STATUS'
]

print("Identifier columns:", identifier_cols)
print("Date columns:", date_cols)
print("Numerical columns:", numerical_cols)
print("Categorical columns:", categorical_cols)

"""2.2 Sanity Check (NO column left behind)"""

# Check if all columns are accounted for
all_listed_cols = (
    identifier_cols +
    date_cols +
    numerical_cols +
    categorical_cols
)

missing_cols = set(df.columns) - set(all_listed_cols)

print("Columns not categorized:", missing_cols)

"""## Dataset Overview

This dataset represents retail banking customers with loan-related attributes,
transaction behavior, demographic indicators, and risk markers.

### Key Feature Groups:
- **Identifier Columns**: Internal banking identifiers (excluded from modeling)
- **Date Columns**: Used for feature engineering (age, account vintage)
- **Numerical Features**: Financial limits, balances, transaction behavior
- **Categorical Features**: Scheme type, profession, geography, risk rating

The dataset contains intentional missingness in several behavioral variables,
which may itself carry predictive information and will be handled carefully
in subsequent steps.

# 3. Data Quality Audit (Missingness, Duplicates & Banking Sanity)

3.1 Missing Value Summary
"""

# ===============================
# Missing Value Analysis
# ===============================

missing_summary = (
    df.isnull()
      .sum()
      .to_frame(name='Missing_Count')
      .assign(Missing_Percentage=lambda x: (x['Missing_Count'] / len(df)) * 100)
      .sort_values('Missing_Percentage', ascending=False)
)

missing_summary

"""3.2 Visualize Missingness (Top 15)"""

# Plot top 15 columns by missing percentage
plt.figure(figsize=(10, 6))
sns.barplot(
    x=missing_summary.head(15)['Missing_Percentage'],
    y=missing_summary.head(15).index
)
plt.title("Top 15 Columns by Missing Percentage")
plt.xlabel("Missing Percentage (%)")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

"""3.3 Duplicate Record Check"""

# ===============================
# Duplicate Check
# ===============================

duplicate_rows = df.duplicated().sum()
print(f"Number of duplicate rows: {duplicate_rows}")

"""3.4 Key Banking Sanity Checks"""

# ===============================
# Banking Sanity Checks
# ===============================

print("Negative values check:")

for col in numerical_cols:
    if (df[col].dropna() < 0).any():
        print(f"⚠️ Negative values found in {col}")

"""3.5 High-Missing Columns Flag (Programmatic)"""

high_missing_cols = missing_summary[
    missing_summary['Missing_Percentage'] > 80
].index.tolist()

print("Columns with >80% missing values:")
high_missing_cols

"""## Data Quality Observations

- Several behavioral features exhibit high missingness, which is expected in retail banking datasets.
- Missing values likely indicate **non-usage of specific banking products** rather than data errors.
- No corrective action is taken at this stage; missingness will be treated as informative in later steps.
- Data duplication and negative-value checks are performed to ensure structural integrity.

# 4. Target Variable Definition (Loan Uptake Logic)

4.1 Create Target Variable
"""

# ===============================
# Target Variable: Loan Uptake
# ===============================

df['LOAN_UPTAKE'] = np.where(
    (df['SANCT_LIM'] > 0) |
    (df['FONELOAN_OS'].fillna(0) > 0) |
    (df['CC_OS'].fillna(0) > 0),
    1,
    0
)

"""4.2 Target Distribution Check"""

# Distribution of target variable
df['LOAN_UPTAKE'].value_counts(normalize=True) * 100

"""4.3 Sanity Check (Random Sample)"""

# Inspect a few records
df[['SANCT_LIM', 'FONELOAN_OS', 'CC_OS', 'LOAN_UPTAKE']].sample(10, random_state=42)

"""## Target Variable Definition: Loan Uptake

Loan uptake is defined as the presence of any active loan exposure.

A customer is considered to have loan uptake (`LOAN_UPTAKE = 1`) if:
- Sanctioned loan limit is greater than zero, OR
- Outstanding fhone loan balance exists, OR
- Outstanding credit card balance exists.

This approach reflects real-world banking practice where loan ownership
is inferred from active credit exposure rather than explicit labels.

# 5. Missing Value Strategy

5.1 Create Missing Indicators
"""

# ===============================
# Missingness Indicator Features
# ===============================

missing_indicator_cols = [
    'NO_OF_QR_TXNS',
    'NO_OF_POS_TXNS',
    'FD_AMT',
    'CC_OS',
    'FONELOAN_OS',
    'MONTHLY_AVG_CR_AMT',
    'MONTHLY_AVG_DR_AMT',
    'MONTHLY_NO_OF_TXNS',
    'OVD_AMT',
    'LOAN_TENURE_MONTHS',
    'REP_FREQ',
    'COLLATERAL_TYPE'
]

for col in missing_indicator_cols:
    df[f'{col}_MISSING'] = df[col].isnull().astype(int)

# Check indicators
df[[f'{col}_MISSING' for col in missing_indicator_cols]].sum()

"""5.2 Impute Numerical Columns"""

# ===============================
# Numerical Missing Value Imputation
# ===============================

numerical_impute_cols = [
    'NO_OF_QR_TXNS',
    'NO_OF_POS_TXNS',
    'FD_AMT',
    'CC_OS',
    'FONELOAN_OS',
    'MONTHLY_AVG_CR_AMT',
    'MONTHLY_AVG_DR_AMT',
    'MONTHLY_NO_OF_TXNS',
    'OVD_AMT',
    'LOAN_TENURE_MONTHS'
]

df[numerical_impute_cols] = df[numerical_impute_cols].fillna(0)

"""5.3 Impute Categorical Columns"""

# ===============================
# Categorical Missing Value Imputation
# ===============================

categorical_impute_cols = [
    'REP_FREQ',
    'COLLATERAL_TYPE'
]

df[categorical_impute_cols] = df[categorical_impute_cols].fillna('None')

"""5.4 Validation Check"""

# Confirm no missing values remain in modeling features
df[numerical_impute_cols + categorical_impute_cols].isnull().sum()

"""## Missing Value Treatment Strategy

Missing values in this dataset primarily represent non-usage of specific
banking products rather than data quality issues.

To preserve this information:
- Missingness indicators were created for key behavioral variables.
- Numerical missing values were imputed with zero to represent absence of activity.
- Categorical missing values were labeled as "None" to retain semantic meaning.

This dual approach ensures that both the absence of a feature and its
informational impact are captured by the model.

# 6. Feature Engineering

6.1 Customer Age (in years)
"""

# ===============================
# Feature Engineering: Age
# ===============================

df['AGE'] = (
    (df['AS_ON'] - df['CUST_DOB']).dt.days / 365.25
).astype(int)

df['AGE'].describe()

"""6.2 Account Vintage (in months)"""

# ===============================
# Feature Engineering: Account Vintage
# ===============================

df['ACCOUNT_VINTAGE_MONTHS'] = (
    (df['AS_ON'] - df['ACCT_OPN_DATE']).dt.days / 30
).astype(int)

df['ACCOUNT_VINTAGE_MONTHS'].describe()

"""6.3 Risk Rating Bucketing"""

# ===============================
# Risk Rating Buckets
# ===============================

df['RISK_BUCKET'] = df['RISK_RATING'].map({
    'LOW': 'Low',
    'MEDIUM': 'Medium',
    'HIGH': 'High'
}).fillna('Unknown')

df['RISK_BUCKET'].value_counts()

"""6.4 Optional Behavioral Flags"""

# ===============================
# Behavioral Flags
# ===============================

df['HAS_QR_USAGE'] = (df['NO_OF_QR_TXNS'] > 0).astype(int)
df['HAS_POS_USAGE'] = (df['NO_OF_POS_TXNS'] > 0).astype(int)
df['HAS_FD'] = (df['FD_AMT'] > 0).astype(int)
df['HAS_CC'] = (df['CC_OS'] > 0).astype(int)

"""## Feature Engineering

Additional features were engineered to enhance model interpretability
and predictive power:

- **Age**: Derived from date of birth
- **Account Vintage**: Duration of customer relationship with the bank
- **Risk Buckets**: Simplified credit risk categorization
- **Behavioral Flags**: Indicators of product usage and transaction behavior

These engineered features reflect real-world banking decision factors.

# 7. Data Cleansing Rules

7.1 Age Validation & Correction
"""

# ===============================
# Age Cleaning
# ===============================

# Define valid age range for retail banking
MIN_AGE = 18
MAX_AGE = 100

df['AGE_CLEAN'] = df['AGE'].clip(lower=MIN_AGE, upper=MAX_AGE)

# Compare before & after
df[['AGE', 'AGE_CLEAN']].describe()

"""7.2 Account Vintage Validation"""

# ===============================
# Account Vintage Cleaning
# ===============================

df['ACCOUNT_VINTAGE_MONTHS_CLEAN'] = df['ACCOUNT_VINTAGE_MONTHS'].clip(lower=0)

df[['ACCOUNT_VINTAGE_MONTHS', 'ACCOUNT_VINTAGE_MONTHS_CLEAN']].describe()

"""7.3 Replace Original Features"""

# Replace original engineered features
df.drop(columns=['AGE', 'ACCOUNT_VINTAGE_MONTHS'], inplace=True)

df.rename(columns={
    'AGE_CLEAN': 'AGE',
    'ACCOUNT_VINTAGE_MONTHS_CLEAN': 'ACCOUNT_VINTAGE_MONTHS'
}, inplace=True)

"""## Data Cleansing Rules

Certain engineered features exhibited values outside plausible business ranges:

- **Age** values below 18 or above 100 were clipped to valid retail banking limits.
- **Account Vintage** values below zero were clipped to zero.

These corrections preserve dataset size while enforcing realistic constraints,
ensuring model stability without introducing bias from record removal.

# 8. Feature Selection & Encoding

8.1 Drop Non-Modeling Columns
"""

# ===============================
# Drop Non-Modeling Columns
# ===============================

drop_cols = identifier_cols + date_cols + ['RISK_RATING']

df_model = df.drop(columns=drop_cols)

df_model.shape

"""8.2 Separate Target"""

# ===============================
# Separate Features & Target
# ===============================

X = df_model.drop(columns=['LOAN_UPTAKE'])
y = df_model['LOAN_UPTAKE']

print("X shape:", X.shape)
print("y shape:", y.shape)

"""8.3 Identify Categorical Columns"""

# Identify categorical columns in X
X_categorical_cols = X.select_dtypes(include='object').columns.tolist()
X_categorical_cols

"""8.4 One-Hot Encoding"""

# ===============================
# One-Hot Encoding
# ===============================

X_encoded = pd.get_dummies(
    X,
    columns=X_categorical_cols,
    drop_first=True
)

X_encoded.shape

"""## Feature Encoding

Categorical variables were encoded using one-hot encoding to convert
non-numeric features into a machine-learning compatible format.

To avoid multicollinearity, the first category was dropped for each
categorical variable.

The final feature matrix includes:
- Numerical features
- Behavioral indicators
- Missingness indicators
- Encoded categorical variables

# 9. Train-Test Split & Feature Scaling

9.1 Train-Test Split
"""

# ===============================
# Train-Test Split
# ===============================

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_encoded,
    y,
    test_size=0.25,
    random_state=42,
    stratify=y
)

print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)

"""9.2 Feature Scaling (Numerical Only)"""

# ===============================
# Feature Scaling
# ===============================

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""9.3 Scaling Validation"""

# Convert back to DataFrame for inspection
X_train_scaled_df = pd.DataFrame(
    X_train_scaled,
    columns=X_train.columns,
    index=X_train.index
)

X_train_scaled_df.describe().T[['mean', 'std']].head()

"""## Train-Test Split & Scaling

The dataset was split into training (75%) and testing (25%) subsets using
stratified sampling to preserve loan uptake distribution.

Feature scaling was applied using StandardScaler, fitted only on the
training data and then applied to the test data to prevent data leakage.

The resulting datasets are fully prepared for model training.

# 10. Baseline Model - Logistic Regression

10.1 Train Logistic Regression Model
"""

# ===============================
# Logistic Regression Model
# ===============================

from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression(
    max_iter=1000,
    n_jobs=-1,
    random_state=42
)

log_reg.fit(X_train_scaled, y_train)

"""10.2 Predictions"""

# Predictions
y_train_pred = log_reg.predict(X_train_scaled)
y_test_pred = log_reg.predict(X_test_scaled)

"""10.3 Model Evaluation"""

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print("Training Accuracy:", accuracy_score(y_train, y_train_pred))
print("Test Accuracy:", accuracy_score(y_test, y_test_pred))

print("\nClassification Report (Test Data):")
print(classification_report(y_test, y_test_pred))

"""10.4 Confusion Matrix Visualization"""

plt.figure(figsize=(6,4))
sns.heatmap(
    confusion_matrix(y_test, y_test_pred),
    annot=True,
    fmt='d',
    cmap='Blues'
)
plt.title("Confusion Matrix - Logistic Regression")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""## Baseline Model: Logistic Regression

A Logistic Regression model was trained as a baseline classifier for loan
uptake prediction.

This model serves as a reference point due to its interpretability and
widespread adoption in banking and regulatory environments.

Performance metrics indicate how well customer loan uptake behavior can be
explained using engineered demographic, behavioral, and product-based features.

# 11. Leakage Analysis & Feature Redesign

## Baseline Model Observation: Near-Perfect Accuracy

The Logistic Regression baseline model achieved near-perfect performance.
While this indicates strong separability, such results are atypical in real-world
loan uptake prediction problems.

This behavior occurs because the target variable (loan uptake) is directly
derived from several input features such as sanctioned limit and outstanding
loan balances. As a result, the model is effectively identifying existing loan
ownership rather than predicting future loan uptake.

To address this, a revised modeling approach will exclude loan-defining features
and focus only on pre-loan behavioral and demographic variables.

#12. Feature Set Redesign for Future Loan Uptake Prediction

12.1 Define Leaky Columns to Drop
"""

# ===============================
# Leakage-Safe Feature Selection (Robust)
# ===============================

leaky_cols = [
    'SANCT_LIM',
    'FONELOAN_OS',
    'CC_OS',
    'LOAN_TENURE_MONTHS',
    'COLLATERAL_TYPE',
    'SANCT_LIM_MISSING',
    'FONELOAN_OS_MISSING',
    'CC_OS_MISSING',
    'LOAN_TENURE_MONTHS_MISSING',
    'COLLATERAL_TYPE_MISSING',
    'HAS_CC'
]

# Keep only columns that actually exist
leaky_cols_existing = [col for col in leaky_cols if col in df_model.columns]

print("Dropping leaky columns:")
print(leaky_cols_existing)

df_future = df_model.drop(columns=leaky_cols_existing)

df_future.shape

"""12.2 Separate Features & Target"""

X_future = df_future.drop(columns=['LOAN_UPTAKE'])
y_future = df_future['LOAN_UPTAKE']

print("X_future shape:", X_future.shape)
print("y_future shape:", y_future.shape)

"""12.3 Identify Categorical Columns"""

X_future_categorical = X_future.select_dtypes(include='object').columns.tolist()
X_future_categorical

"""12.4 One-Hot Encoding"""

X_future_encoded = pd.get_dummies(
    X_future,
    columns=X_future_categorical,
    drop_first=True
)

X_future_encoded.shape

"""## Feature Redesign for Future Loan Uptake Prediction

To align the model with the objective of predicting *future* loan uptake,
all loan-defining and post-loan variables were removed from the feature set.

The revised feature universe includes only demographic, behavioral, and
relationship attributes that are observable prior to loan acquisition.

This redesign ensures that the model learns genuine predictive patterns
rather than identifying existing loan ownership.

# 13. Train-Test Split & Baseline Model (Future Loan Uptake)

13.1 Train-Test Split
"""

from sklearn.model_selection import train_test_split

Xf_train, Xf_test, yf_train, yf_test = train_test_split(
    X_future_encoded,
    y_future,
    test_size=0.25,
    random_state=42,
    stratify=y_future
)

print("Train shape:", Xf_train.shape)
print("Test shape:", Xf_test.shape)

"""13.2 Scaling"""

from sklearn.preprocessing import StandardScaler

scaler_future = StandardScaler()

Xf_train_scaled = scaler_future.fit_transform(Xf_train)
Xf_test_scaled = scaler_future.transform(Xf_test)

"""13.3 Logistic Regression"""

from sklearn.linear_model import LogisticRegression

log_reg_future = LogisticRegression(
    max_iter=1000,
    n_jobs=-1,
    random_state=42
)

log_reg_future.fit(Xf_train_scaled, yf_train)

"""13.4 Evaluation"""

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

yf_test_pred = log_reg_future.predict(Xf_test_scaled)

print("Test Accuracy:", accuracy_score(yf_test, yf_test_pred))
print("\nClassification Report (Future Model):")
print(classification_report(yf_test, yf_test_pred))

"""13.6 Confusion Matrix"""

plt.figure(figsize=(6,4))
sns.heatmap(
    confusion_matrix(yf_test, yf_test_pred),
    annot=True,
    fmt='d',
    cmap='Greens'
)
plt.title("Confusion Matrix – Future Loan Uptake Model")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""## Future Loan Uptake Model – Baseline Results

This baseline Logistic Regression model was trained using only pre-loan
demographic, behavioral, and relationship features.

Unlike the earlier identification model, performance metrics here reflect
a realistic prediction task. The results represent the model’s true ability
to forecast potential loan uptake rather than detect existing loan ownership.

This model serves as a reliable benchmark for further improvement using
advanced algorithms and feature refinement.

# 14. Model Evaluation, Strengths & Limitations

## Model Evaluation Summary

The future loan uptake prediction model demonstrates strong predictive
performance using only pre-loan customer attributes.

Key observations:
- High overall accuracy indicates effective separation between potential
  loan adopters and non-adopters.
- High recall for loan uptake suggests the model successfully identifies
  customers likely to take loans.
- Low false positive rate minimizes unnecessary loan marketing efforts.

The results confirm that customer behavior and relationship history are
strong indicators of future loan demand.

## Strengths of the Model

- Uses only pre-loan features, ensuring no information leakage.
- Highly interpretable due to the use of Logistic Regression.
- Suitable for regulatory and banking environments.
- Strong baseline performance without complex tuning.
- Scalable and easy to deploy in production systems.

## Limitations of the Model

- Logistic Regression assumes linear relationships between features and target.
- High-dimensional one-hot encoded features may reduce interpretability.
- Temporal dynamics are not explicitly modeled.
- Model does not account for external economic or seasonal factors.
- Performance may degrade over time due to customer behavior drift.

## Business Implications

The model can be used to:
- Identify customers with high likelihood of future loan uptake.
- Prioritize targeted marketing and cross-selling campaigns.
- Support relationship managers with data-driven insights.

However, predictions should be used as decision-support tools rather than
automated approval mechanisms.

# 15. Pre-Deployment Considerations

## Pre-Deployment Checklist

Before deploying the loan uptake prediction model, the following steps
must be completed:

- Independent model validation and peer review
- Data pipeline validation to ensure consistency with training data
- Feature stability and drift analysis
- Approval from risk and compliance teams
- Security and access control review

## Monitoring and Retraining Strategy

Post-deployment, the model must be continuously monitored for:

- Prediction accuracy and recall over time
- Data drift in key behavioral features
- Concept drift due to changing customer behavior
- Bias across demographic and regional segments

Scheduled retraining should be performed periodically using recent data
to maintain model relevance and accuracy.

## Ethical and Regulatory Considerations

- Customer data must be handled in compliance with data protection regulations.
- Model decisions should be explainable and transparent.
- Sensitive attributes must not result in discriminatory outcomes.
- Human oversight should be maintained for critical decisions.

The model is intended as a decision-support tool, not a replacement for
human judgment.

## Conclusion

This project developed a robust and defensible model to predict future
loan uptake using retail banking customer data.

By carefully handling missing data, avoiding information leakage, and
focusing on pre-loan attributes, the model delivers realistic and
actionable insights.

The resulting framework provides a strong foundation for further
enhancement using advanced models and time-aware features.
"""